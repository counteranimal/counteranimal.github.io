<!DOCTYPE html>
<!-- TypeIt package -->
<script src="https://code.jquery.com/jquery-3.0.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/jquery.typeit/4.4.0/typeit.min.js"></script>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="description" content="Do CLIPs Always Generalize Better than ImageNet Models?">
        <meta name="keywords" content="JailBreak, LLM, Security">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Do CLIPs Always Generalize Better than ImageNet Models?</title>
        <!-- Google tag (gtag.js) -->
        <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-MK2R9XDD88"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());

            gtag('config', 'G-MK2R9XDD88');
        </script> -->
        <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
        <link rel="stylesheet" href="./static/css/bulma.min.css">
        <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
        <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <link rel="stylesheet" href="./static/css/index.css">
        <link rel="icon" href="./static/images/ice_bear.png">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>
        <script src="./static/js/bulma-carousel.min.js"></script>
        <script src="./static/js/bulma-slider.min.js"></script>
        <script src="./static/js/index.js"></script>
        <!-- Typing Effect JS -->
        <script src="https://code.jquery.com/jquery-3.0.0.min.js"></script>
        <script src="https://cdn.jsdelivr.net/jquery.typeit/4.4.0/typeit.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/typeit/5.10.1/typeit.min.js"></script>
        <!-- / Typing Effect JS -->
        <style>
    .bigdiv {
      font-size: large;
      font-family: "Courier New";
      padding: 2rem;
    }
    p {
      padding: 2rem;
    }
        </style>
    </head>
    <body>
        <section class="hero">
            <div class="hero-body">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered">
                            <div class="column has-text-centered">
                                <img src="./static/images/ice_bear.png" style="width:110px; float: left;">
                            </div>
                            <h1 class="title is-1 publication-title">Do CLIPs Always Generalize Better than ImageNet Models?</h1>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block">
                                    <a href="https://qizhouwang.github.io/homepage/">Qizhou Wang</a>
                                    <sup>1*</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <a href="https://linyongver.github.io/PersonalWebsite/">Yong Lin</a>
                                    <sup>2*</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <a href="https://lfhase.win">Yongqiang Chen</a>
                                    <sup>3*</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a>
                                    <sup>4</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <a href="https://bhanml.github.io/">Bo Han</a>
                                    <sup>1</sup>
                                    ,
                                </span>
                                <span class="author-block">
                                    <a href="https://tongzhang-ml.org/">Tong Zhang</a>
                                    <sup>5</sup>
                                </span>
                            </div>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block">
                                    <sup>1</sup>Hong Kong Baptist University
                                </span><br>
                                <span class="author-block">
                                    <sup>2</sup>The Hong Kong University of Science and Technology
                                </span><br>
                                <span class="author-block">
                                    <sup>3</sup>The Chinese University of Hong Kong
                                </span>
                                <br>
                                <span class="author-block">
                                    <sup>4</sup>The University of Washington
                                </span>
                                <br>
                                <span class="author-block">
                                    <sup>5</sup> University of Illinois Urbana-Champaign
                                </span>
                            </div>
                            <div class="is-size-5 publication-authors">
                                <span class="author-block" style="font-size: 15px;">(
                                    <sup>*</sup>Equal Contribution)
                                </span>
                            </div>
                            <div class="column has-text-centered">
                                <div class="publication-links">
                                    <!-- PDF Link. -->
                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/2403.11497" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fas fa-file-pdf"></i>
                                            </span>
                                            <span>Paper</span>
                                        </a>
                                    </span>
                                    <!-- Code Link. -->
                                    <span class="link-block">
                                        <a href="https://drive.google.com/file/d/1SrPF0rgXKy_4z1kXsIYJt-2puM78LbAG/view?usp=sharing" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fab fa-github"></i>
                                            </span>
                                            <span>Dataset</span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
            

            <section class="section">
                <div class="container is-max-desktop">
                    <!-- Abstract. -->
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <h2 class="title is-3">Abstract</h2>
                            <div class="content has-text-justified">
                                <p>
                                    Large vision language models, such as CLIPs, have revolutionized modern machine learning. CLIPs have demonstrated great
                                    generalizability under distribution shifts, supported by an increasing body of literature. However, the evaluation
                                    datasets for CLIPs are variations primarily designed for ImageNet benchmarks, which may not fully reflect the extent to
                                    which CLIPs, e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap, we collect a real-world
                                    dataset called CounterAnimal that contains realistic spurious features found in animal photos. CounterAnimal consists of
                                    a) the common group: comprising animals on common backgrounds, and b) the counter group: including animals on unusual
                                    backgrounds. The performance drops from common to counter quantify the reliance of models on spurious features (i.e.,
                                    backgrounds) to predict the animals. We find that CLIPs trained on either LAION or the OpenAI data exhibit notable
                                    performance drops on counter. Surprisingly, we observe that single-modal models trained on ImageNet are more robust than
                                    CLIPs. We provide both theoretical and empirical explanations for why CLIPs still learn spurious features. Our findings
                                    suggest that distribution shifts remain an open problem for CLIPs, and one needs to be cautious about test setups when
                                    evaluating foundation models pre-trained on a significantly different scale and distribution.
                                </p>
                            </div>
                        </div>
                    </div>
                    <!--/ Abstract. -->
                </div>
            </section>
            
            <section class="section">
                <div class="container is-max-desktop">
                    <div class="column is-full-width has-text-centered">
                        <h2 class="title is-3">CounterAnimal Dataset</h2>
                        
                        
            
                        <div class="content has-text-justified is-centered">
                            <p>
                                Previous works often claim that CLIPs can resolve the distribution shifts, but their adopted datasets to test the effective robustness 
                                are primarily designed for the ImageNet-based models. It may not fully reflect the cases of CLIPs. Therefore, we collect a real-world 
                                dataset named CounterAnimal, containing spurious features for animal photos. Data therein are separated into two groups:
                            </p>
                            <ul>
                                <li>
                                    <b>The common group</b> contains photos of animals in frequently appeared backgrounds.  
                                </li>

                                <li>
                                    <b>The counter group</b> contains photos of animals in less commonly yet still plausibly appeared backgrounds. 
                                </li>
                            </ul>
                            <p>
                                The common group captures some real-world biases that CLIPs-trained on web-scale data may naturally inherit. Therefore, by comparing the performance on the
                                common group against that on the counter group, one can quantify to what extent CLIPs rely on spurious features.  
                            </p>
                        </div>

                        <div class="content has-text-centered">
                            <img src="./static/images/dataset_example_ice_bear.png" alt="img description" style="width:1000px;"
                                class="result-image">
                            <p class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 1</b>. Examplary photos with the object label of <b>ice bear</b>, further separating into
                                <b>common</b> and <b>counter</b>
                                groups based on different backgrounds (i.e., snow and grass). The performance
                                drop in zero-shot inference by CLIP-LAION400M-ViT/B/32 is significant, from common (97.62%) to counter (70.91%).
                            </p>
                        </div>

                        <div class="content has-text-justified is-centered">
                            <p>
                                We take <a href="https://www.inaturalist.org/observations">iNaturalist</a> as our raw data source, manually labeling objects (i.e., animal names)
                                and backgrounds (e.g., grass and snow) to find spurious features that can hinder the robustness of CLIPs. The resulting dataset has <b>7,174</b> common photos
                                and <b>5,926</b> counter photos, covering a total of <b>45</b> animal classes as a subset of the ImageNet-1K label space. Our dataset can be downloaded 
                                by the following <a href="https://drive.google.com/file/d/1SrPF0rgXKy_4z1kXsIYJt-2puM78LbAG/view?usp=sharing">link</a>. We further summarize the 
                                object names as well as the background names for common and counter parts.
                            </p>
                        </div>

                        <div class="content has-text-centered">
                            <img
                                src="./static/images/table_names.png"
                                alt="img description"
                                style="width:1000px;"
                                class="result-image"
                            >
                            <p class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Table 1</b>. The full object names as well as the background names of common and counter groups. The full names of the labels
                                are presented following the fashion of the ImageNet-1K dataset. 
                            </p>
                        </div>
                    </div>
                </div>
            </section>
        <section class="section">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full-width has-text-centered">
                        <h2 class="title is-3">Results on our CounterAnimal Dataset</h2>
                        <div class="content has-text-justified is-centered">
                            <p>
                                We evaluate various learning schemes on CounterAnimal and employ two evaluation setups to fit different learning setups: 
                            </p>

                            <ul>
                                <li>
                                    <b>1 vs 1000 setup:</b> using the full ImageNet-1K class names as the candidate label space. 
                                </li>
                                <li>
                                    <b>1 vs 20 setup:</b> using the top-20 most confusing classes for the CLIP model as the candidate label space. 
                                </li>
                            </ul>
                            <p>
                                We summarize the common and counter performance on counteranimal across various learning schemes, including CLIP pre-trained 
                                models, ImageNet models, and more advanced LVLMs such as MiniGPT4 and LLaVA. Our experiments also consider various pre-train
                                datasets for CLIPs and various backbones models. 
                            </p>
                        </div>

                        <div class="content has-text-centered">
                            <img
                                src="./static/images/full_results.png"
                                style="width:850px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <p class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 2</b>. The common vs counter performance for CLIPs, ImageNet models, and more advanced LVLMs. The size of markers
                                indicates the backbone scales and the color shade of markers indicates the pre-train dataset scales. We depict the prefect trend 
                                (i.e., y=x) where models donot learn any spurious correlations, linearly fit the trends for CLIPs and ImageNet models; highlight 
                                the CLIP models pre-trained on two high-quality datasets, i.e., DataComp (CLIP-DC) and DFN (CLIP-DFN). 
                            </p>
                        </div>
                        <div class="content has-text-justified is-centered">
                            <p>
                                <b>We highlight some of the key observations as follows. </b>
                                <br><br>
                                <b>CLIP models still learn spurious correlations.</b> We observe that CLIP models exhibit non-trivial performance drop from common
                                to counter groups. It implies that CounterAnimal characterizes some general spurious correlations that are commonly present in 
                                large-scale real-world multimodal datasets. Moreover, CLIP may still learn the spurious features in real-world pre-train datasets. 
                                We present several examples of the CLIP models for the performance drop. 
                            </p>
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/images/performance_drop.png"
                                style="width:650px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <p class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 3</b>. Comparison of the common and counter groups for varying CLIP setups: 
                                a) Fixing the pre-train dataset to be LAION400M and b) fixing the backbone to be ViT/B/32. 
                                The 1 vs 1000 results are given.
                            </p>
                        </div>
                        <div class="content has-text-justified is-centered">
                            <p>
                                <b>ImageNet models are more robust to spurious features in CounterAnimal.</b>  Compared with CLIPs, we find that 
                                ImageNet models exhibit better robustness to the presented spurious correlations. It is evident from Figure 2 by the superior performance 
                                of ImageNet models on the counter groups and similar common performance with respect to CLIPs. Our findings contrast with previous
                                studies that evaluated distribution shifts using ImageNet variants, indicating that the CLIP does not necessarily generalize 
                                better than ImageNet models. We illustrate two comparison with fixed backbones as follows. 
                            </p>
                        </div>

                        <div class="content has-text-centered">
                            <img
                                src="./static/images/clip and imagenet.png"
                                style="width:650px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <p class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 4</b>. Comparison between CLIPs and ImageNet Models. The 1 vs. 1000 results are given. 
                            </p>
                        </div>
                        
                        <div class="content has-text-justified is-centered">
                            <p>
                                <b>Larger CLIPs are more robust.</b> in Figure 2, we use sizes and color shades of markers to indicate 
                                the scales of backbones and pre-train datasets, respectively. Overall, larger CLIP backbones (i.e., larger markers) can improve the
                                counter performance, implying that scaling up backbones may enhance robustness. In contrast, increasing the scale of the 
                                pre-train dataset (i.e., darker markers) does not yield the same improvement, implying that collecting more data alone cannot rectify much bias.
                                We present several comparison with either fixed pre-train datasets and fixed backbones as follows.
                            </p>
                        </div>

                        <div class="content has-text-centered">
                            <img
                                src="./static/images/fixed_backbones.png"
                                style="width:650px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <p class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 5</b>. Comparison with different pre-train datasets and fixed backbones. 1 vs. 1000 results are given. 
                            </p>
                        </div>

                        <div class="content has-text-centered">
                            <img
                                src="./static/images/fixed_datasets.png"
                                style="width:650px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                            <p class="content is-centered" style="color: gray; font-size: 10pt;">
                                <b>Figure 6</b>. Comparison with different backbones and fixed pre-train datasets. 1 vs. 1000 results are given. 
                            </p>
                        </div>

                        <div class="content has-text-justified is-centered">
                            <p>
                                <b>CLIP model pre-trained on high-quality data are more robust.</b> We discern two classes of CLIPs in Figure 2, a) those 
                                pre-trained on high-quality datasets, i.e., DataComp (CLIP-DC) and {DFN}~\citep{fang2023data} (CLIP-DFN), and b) those 
                                pre-trained on other datasets that lack stringent curation (CLIP). As we can see, CLIPs pre-trained on high-quality data 
                                generally exhibit superior robustness, suggesting that improving data quality is still a promising way against spurious features. 
                            </p>
                        </div>

                        <div class="content has-text-justified is-centered">
                            <p>
                                <b>CLIP objective may not offer additional robustness.</b> To better understand the observed phenomena, we present a theoretical 
                                analysis of why CLIPs rely on spurious features. 
                            </p>
                        </div>
                        <div class="content has-text-centered">
                            <img
                                src="./static/images/theory.png"
                                style="width:900px;"
                                class="result-image"
                                alt="Interpolation end reference image."
                            >
                        </div>
                        <div class="content has-text-justified is-centered">
                            <p>
                                Theorem 1 implies that, once there exists a relatively strong correlation between the object 
                                captions and the parts of image backgrounds, CLIPs will learn to align the backgrounds, i.e., spurious features, with object 
                                captions. The theoretical analysis also align with our empirical findings. 
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section class="section" id="BibTeX">
            <div class="container is-max-desktop content">
                <h2 class="title">Contact</h2>
                <p>
                    Welcome to check our paper for more details of the research work. If there is any question, please feel free to contact us.
                </p>
                <p>
                    If you find our paper and repo useful, please consider to cite:
                </p>
                <pre>
                    <code>
    @article{causalcoat2024,
      title={Do CLIPs Always Generalize Better than ImageNet Models?}, 
      author={Qizhou Wang and Yong Lin and Yongqiang Chen and Ludwig Schmidt and Bo Han and Tong Zhang},
      year={2024},
      journal = {arXiv preprint},
      volume = {arXiv:XXXXXXX}
    }
                    </code>
                </pre>
                <br>
            </div>
        </section>
        <footer class="footer">
            <div class="container">
                <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
                <div class="columns is-centered">
                    <div class="column is-8">
                        <div class="content">
                            <p>
                                Thanks for the source template from
                                <a href="https://github.com/nerfies/nerfies.github.io">here</a>
                                .
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
